Scalable Parallel Execution.
El kernel específica las declaraciones C ejecutadas por cada hilo. A medida que desencadenan una actividad de ejecución tan masiva, necesitamos controlar estas actividades para lograr los resultados, la eficiencia y la velocidad deseados. En este capítulo, estudiaremos conceptos importantes involucrados en el control de la ejecución paralela. Comenzaremos aprendiendo como el índice de subprocesos y el índice de bloques pueden facilitar el procesamiento de matrices multidimensionales.

CUDA Thread Organization.
Todos los subprocesos CUDA en una cuadrícula ejecutan la misma función del núcleo; dependen de coordenadas para distinguirse entre sí e identificar la porción apropiada de datos para procesar. Estos subprocesos están organizados en una jerarquía de dos niveles: una cuadrícula consta de uno o más bloques y cada bloque consta de uno o más subprocesos. Todos los subprocesos de un bloque comparten el mismo índice de bloque, que es el valor de la variable blockIdx en un kernel. Cada hilo tiene un índice de hilo, al que se puede acceder como el valor de la variable threadIdx en un kernel.
En general, una cuadrícula es una matriz tridimensional de bloques y cada bloque es una matriz tridimensional de subprocesos. Al iniciar un kernel, el programa necesita especificar el tamaño de la cuadrícula y los bloques en cada dimensión. El programador puede usar menos de tres dimensiones estableciendo el tamaño de las dimensiones estableciendo el tamaño de las dimensiones no utilizadas en 1. La organización exacta de una cuadrícula está determinada por los parámetros de configuración de ejecución (dentro de <<<>>>) de la declaración de lanzamiento del kernel.
El número de bloques puede variar con el tamaño de los vectores para que la cuadrícula tenga suficientes subprocesos para cubrir todos los elementos del vector. El valor de la variable n en el momento del lanzamiento del kernel determinará la dimensión de la cuadrícula.
Para mayor comodidad, CUDA C proporciona un atajo especial para iniciar un kernel con cuadrículas y bloques unidimensionales. En lugar de variables dim3, se pueden usar expresiones aritméticas para especificar la configuración de cuadrículas y bloques 1D.
En cuanto a la configuración de bloques, cada bloque está organizado en una matriz tridimensional de subprocesos. Se pueden crear bloques bidimensionales configurando blockDim.z en 1. Se pueden crear bloques unidimensionales configurando blockDim.y y blockDim.z en 1.
El tamaño total de un bloque está limitado a 1024 subprocesos, con flexibilidad para distribuir estos elementos en las tres dimensiones siempre que el número total de subprocesos no supere los 1024.

Hierarchical Organizations.
Al igual que los subprocesos CUDA, muchos sistemas del mundo real están organizados jerárquicamente. El sistema telefónico de Estados Unidos es un buen ejemplo. En el nivel superior, el sistema telefónico consta de “áreas”, cada una de las cuales corresponde a un área geográfica.
Cada línea telefónica puede considerarse como un subproceso CUDA, el código de área como el valor de blockIdx y el número local de siete dígitos como el valor de threadIdx. Esta organización jerárquica permite que el sistema admite un número considerablemente grande de líneas telefónicas preservando al mismo tiempo la “localidad” para llamar a la misma área. Al marcar una línea telefónica en la misma área, la persona que llama solo necesita marcar el número local.

Mapping Threads To Multidimensional Data.
La elección de organizaciones de subprocesos 1D, 2D o 3D suele basarse en la naturaleza de los datos. Las imágenes son una matriz 2D de píxeles. El uso de una cuadrícula 2D que consta de bloques 2D suele ser conveniente para procesar los píxeles de una imagen.
Tenga en cuenta que tenemos 4 subprocesos adicionales en la dirección x y 2 subprocesos adicionales en la dirección y; es decir, generamos 80 x 64 subprocesos para procesar 76 x 62 píxeles. Este caso es similar a la situación en la que un vector de 1000 elementos es procesado por el núcleo 1D vecAddKernel utilizando cuatro bloques de 256 subprocesos.
Supongamos que el código del host utiliza una variable entera m para rastrear el número de píxeles en la dirección x y otra variable entera n para rastrear el número de píxeles en la dirección y. Además, asumimos que los datos de la imagen de entrada se han copiado a la memoria del dispositivo y se puede acceder a ellos a través de una variable de puntero d_Pin.
Idealmente, nos gustaría acceder a d_Pin como una matriz bidimensional donde se puede acceder a un elemento en la fila j y la columna i como d_Pin[j][i]. Sin embargo, el estándar ANSI C en el que se basó el desarrollo de CUDA C requiere que se conozca el número de columnas en d_Pin en el momento de la compilación para poder acceder a d_Pin como una matriz 2D. Desafortunadamente, esta información no se conoce en el momento del compilador para las matrices asignadas dinámicamente. De hecho, parte de la razón por la que se utilizan matrices asignadas dinámicamente es para permitir que los tamaños y dimensiones de estas matrices varían según el tamaño de los datos en tiempo de ejecución.
En realidad, todas las matrices multidimensionales en C están linealizadas debido al uso de un espacio de memoria “plano” en las computadoras modernas. En matrices asignadas estáticamente, los compiladores permiten a los programadores utilizar una sintaxis de indexación de dimensiones superiores, como d_Pin[j][i], para acceder a sus elementos.
Otro método para linealizar una matriz bidimensional es colocar todos los elementos de la misma columna en ubicaciones consecutivas. Luego, las columnas se colocan una tras otra en el espacio de memoria. Los compiladores de FORTRAN utilizan esta disposición, denominada diseño de columna principal. El diseño de la columna principal de una matriz bidimensional es equivalente al diseño de la fila principal de su forma transpuesta.

Memory Space.
El espacio de memoria es una vista simplificada de cómo un procesador accede a su memoria en las computadoras modernas. Generalmente está asociado con cada aplicación en ejecución. Los datos que una aplicación debe procesar y las instrucciones ejecutadas para la aplicación se almacenan en ubicaciones en su espacio de memoria.
Las ubicaciones en un espacio de memoria son similares a los teléfonos en un sistema telefónico donde todos tienen un número de teléfono único. La mayoría de las computadoras modernas tienen al menos ubicaciones de tamaño de bytes 4G, donde cada G es 1.073.741.824 (2^30). Todas las ubicaciones están etiquetadas con una dirección que va del 0 al número más grande.

Image Blur: A More Complex Kernel.
Hemos estudiado vecAddkernel y colorToGreyscaleConversion en los que cada hilo realiza solo una pequeña cantidad de operaciones aritméticas en un elemento de la matriz. Estos núcleos cumplen bien su propósito: ilustrar la estructura básica del programa CUDA C y los conceptos de ejecución paralela de datos.
El desenfoque de la imagen suaviza la variación abrupta de los valores de los píxeles y al mismo tiempo preserva los bordes que son esenciales para reconocer las características clave de la imagen.En pocas palabras, hacemos que la imagen aparezca borrosa. Para el ojo humano, una imagen borrosa tiende a oscurecer los detalles finos y presentar la impresión de “panorama general” o los objetos temáticos principales de la imagen. En los algoritmos de procesamiento de imágenes por computadora, un caso de uso común de desenfoque de imágenes es reducir el impacto del ruido y los efectos de representación granular en una imagen corrigiendo los valores de píxeles problemáticos con los valores limpios de los píxeles circundantes.
Matemáticamente, una función de desenfoque de imagen calcula el valor de un píxel de la imagen de salida como una suma ponderada de un parche de píxeles que abarca el píxel de la imagen de entrada.

Synchronization And Transparent Scalability.
CUDA permite que los subprocesos del mismo bloque coordinen sus actividades mediante el uso de una función de sincronización de barrera __syncthreads(). Cuando un hilo llama a __syncthreads(), se mantendrá en la ubicación de llamada hasta que cada hilo del bloque llegue a esa ubicación. Este proceso garantiza que todos los subprocesos de un bloque hayan completado una fase de ejecución del kernel antes de que cualquiera de ellos pueda pasar a la siguiente fase. 
La sincronización de barreras es un método simple y popular para coordinar actividades paralelas. En la vida real, a menudo utilizamos la sincronización de barreras para coordinar actividades paralelas de varias personas. 
En CUDA, todos los subprocesos de un bloque deben ejecutar una instrucción __syncthreads(), si está presente. Cuando una declaración __syncthreads() se coloca en una declaración if, todos o ninguno de los subprocesos en un bloque ejecutan la ruta que incluye __syncthreads(). Para una declaración if-then-else, si cada ruta tiene una declaración __syncthreads(), todos los subprocesos de un bloque ejecutan la ruta then o todos ejecutan la ruta else. Los dos __syncthreads() son puntos de sincronización de barrera diferentes.
La capacidad de sincronizar también impone restricciones de ejecución en los subprocesos dentro de un bloque. Estos subprocesos deben ejecutarse en estrecha proximidad temporal entre sí para evitar tiempos de espera excesivamente largos. De hecho, es necesario asegurarse de que todos los subprocesos involucrados en la sincronización de la barrera tengan acceso a los recursos necesarios para llegar finalmente a la barrera. De lo contrario, un hilo que nunca llega al punto de sincronización de barrera puede hacer que todos los demás esperen para siempre.
Esto nos lleva a una importante compensación en el diseño de la sincronización de barrera CUDA. Al no permitir que los subprocesos en diferentes bloques realicen sincronización de barrera entre sí, el sistema de tiempo de ejecución de CUDA puede ejecutar bloques en cualquier orden entre sí porque ninguno de ellos necesita esperar el uno al otro.
La capacidad de ejecutar el mismo código de aplicación dentro de una amplia gama de velocidades permite la producción de una amplia gama de implementaciones de acuerdo con los requisitos de costo, potencia y rendimiento de segmentos particulares del mercado.

Resource Assignment.
Una vez que se inicia un kernel, el sistema de ejecución de CUDA genera la cuadrícula de subprocesos correspondiente. En la generación actual de hardware, los recursos de ejecución están organizados en Streaming Multiprocessors (SM). Cada dispositivo establece un límite en la cantidad de bloques que se pueden asignar a cada SM. En situaciones en las que hay escasez de uno o más tipos de recursos necesarios para la ejecución simultánea de 8 bloques, el tiempo de ejecución de CUDA reduce automáticamente la cantidad de bloques asignados a cada SM hasta que su uso combinado de recursos caiga por debajo del límite. Con una cantidad limitada de SM y una cantidad limitada de bloques que se pueden asignar a cada SM, la cantidad de bloques que se pueden ejecutar activamente en un dispositivo CUDA también es limitada.
Una de las limitaciones de recursos de SM es la cantidad de subprocesos que se pueden rastrear y programar simultáneamente. Se necesitan recursos de hardware (registros integrados) para que los SM mantengan los índices de subprocesos y bloques y realicen un seguimiento de su estado de ejecución. Por lo tanto, cada generación de hardware establece un límite en la cantidad de bloques y la cantidad de subprocesos que se pueden asignar a un SM.

Querying Device Properties.
En general, muchas aplicaciones modernas están diseñadas para ejecutarse en una amplia variedad de sistemas de hardware. La aplicación a menudo necesita consultar los recursos y capacidades disponibles del hardware subyacente para aprovechar los sistemas más capaces y al mismo tiempo compensar los sistemas menos capaces.
En CUDA C, existe un mecanismo incorporado para que un código de host consulte las propiedades de los dispositivos disponibles en el sistema. El sistema de tiempo de ejecución CUDA (controlador de dispositivo) tiene una función API cudaGetDeviceCount que devuelve la cantidad de dispositivos CUDA disponibles en el sistema.
Si bien puede que no sea obvio, un sistema de PC moderno suele tener dos o más dispositivos CUDA. La razón es que muchos sistemas de PC vienen con una o más GPU “integradas”. Estas GPU son las unidades gráficas predeterminadas y proporcionan capacidades y recursos de hardware rudimentarios para realizar funcionalidades gráficas mínimas para interfaces de usuario modernas basadas en Windows.

Thread Scheduling And Latency Tolerance.
La programación de subprocesos es estrictamente un concepto de implementación. Por lo tanto, debe discutirse en el contexto de implementaciones de hardware específicas. En la mayoría de las implementaciones hasta la fecha, un bloque asignado a un SM se divide en 32 unidades de subprocesos llamadas deformaciones.
El warp es la unidad de programación de hilos en los SM. Cada warp consta de 32 hilos de valores threadIdx consecutivos: los hilos del 0 al 31 forman el primer warp, del 32 al 63 el segundo warp, y así sucesivamente.
Un SM está diseñado para ejecutar todos los subprocesos en un warp siguiendo el modelo de instrucción única, datos múltiples (SIMD), es decir, en cualquier instante, se recupera y ejecuta una instrucción para todos los subprocesos en el warp.
Cuando una instrucción que va a ser ejecutada por un warp necesita esperar el resultado de una operación de latencia larga iniciada previamente, el warp no se selecciona para su ejecución. En su lugar, se seleccionará para su ejecución otro warp residente que ya no esté esperando resultados.
La programación Warp también se utiliza para tolerar otros tipos de latencias de operación, como la aritmética de punto flotante canalizada y las instrucciones de bifurcación. Dada una cantidad suficiente de warps, el hardware probablemente encontrará un warp para ejecutar en cualquier momento, haciendo así un uso completo del hardware de ejecución a pesar de estas operaciones de larga latencia. La selección de warps listos para su ejecución evita introducir tiempo inactivo o perdido en la línea de tiempo de ejecución, lo que se conoce como programación de subprocesos sin sobrecarga.

Conclusiones.
Con este contenido se reforzó, reafirmó y aclara información revisada en otras ocasiones mostrando un panorama más claro al ser este contenido directamente distribuido por NVIDIA en vez de terceros interpretando o explicando esta información.

Después de revisar la información otorgada sobre programación paralela y CUDA queda claro que es importante un correcto uso de esta ya que puede permitir y mejorar el tiempo de ejecución de las tareas dentro de una aplicación o sistema afinando su optimización y volviendo al propio sistema en sí, más eficiente.
