Data Parallel Computing.
CUDA C amplía el popular lenguaje de programación C con una sintaxis e interfaces mínimamente nuevas para permitir a los programadores apuntar a sistemas informáticos heterogéneos que contienen núcleos de CPU y GPU masivamente paralelas. Como su nombre lo indica, CUDA C se basa en la plataforma CUDA de NVIDIA. CUDA es actualmente el marco más maduro para la computación paralela masiva.
Cuda es en realidad una arquitectura que admite un conjunto de conceptos para organizar y expresar computación masivamente paralela.

Data Parallelism.
Cuando las aplicaciones de software modernas se ejecutan con lentitud, el problema suele ser tener demasiados datos para procesar. Las aplicaciones de consumo manipulan imágenes o vídeos, con millones o billones de píxeles. Las aplicaciones científicas modelan la dinámica de fluidos utilizando miles de millones de celdas de cuadrícula. Las aplicaciones de dinámica molecular deben simular interacciones entre miles y millones de átomos. Es importante destacar que la mayoría de estos píxeles, partículas, células, interacciones, vuelos, etc., se pueden abordar en gran medida de forma independiente. Incluso una operación aparentemente global, como encontrar el brillo promedio de todos los píxeles de una imagen, se puede dividir en muchos cálculos más pequeños que se pueden ejecutar de forma independiente. Esta evaluación independiente es la base del paralelismo de datos: (re)organizar el cálculo en torno a los datos, de modo que podamos ejecutar los cálculos independientes resultantes en paralelo para completar el trabajo general más rápido, a menudo mucho más rápido.

Task Parallelism Vs. Data Parallelism.
El paralelismo de datos no es el único tipo de paralelismo utilizado en la programación paralela. El paralelismo de tareas también se ha utilizado ampliamente en la programación paralela. El paralelismo de tareas suele quedar expuesto a través de la descomposición de tareas de las aplicaciones. El paralelismo de tareas existe si las dos tareas se pueden realizar de forma independiente.
En aplicaciones grandes, suele haber una mayor cantidad de tareas independientes y, por lo tanto, una mayor cantidad de paralelismo de tareas.
En general, el paralelismo de datos es la principal fuente de escalabilidad de los programas paralelos. Con grandes conjuntos de datos, a menudo se puede encontrar abundante paralelismo de datos para poder utilizar procesadores paralelos masivos y permitir el rendimiento de las aplicaciones. El rendimiento crece con cada generación de hardware que tiene más recursos de ejecución.

RGB Color Image Representation.
En una representación RGB, cada píxel de una imagen se almacena como una tupla de valores (r, g, b). Cada tupla especifica una mezcla de rojo (R), verde (G) y azul (B). Es decir, para cada píxel, los valores r, g y b representan la intensidad (0 es oscuro y 1 es intensidad total) de las fuentes de luz roja, verde y azul cuando se representa el píxel. 
Las mezclas reales permitidas de estos tres colores varian segun los espacios de color especificados por la industria. La coordenada vertical (valor y) y la coordenada horizontal (valor x) de cada mezcla muestran la fracción de la intensidad de pixel que debería ser G y R. La fracción restante (1 - y - x) de la intensidad de pixel que debería asignarse a B.

CUDA C Program Structure.
La estructura de un programa CUDA C refleja la coexistencia de un host (CPU) y uno o más dispositivos (GPU) en la computadora. Cada archivo fuente CUDA puede tener una combinación de código de host y de dispositivo. De forma predeterminada, cualquier programa C tradicional es un programa CUDA que contiene solo código de host. Las funciones o declaraciones de datos del dispositivo están claramente marcadas con palabras clave CUDA C especiales.
Una vez que las funciones del dispositivo y las declaraciones de datos se agregan a un archivo fuente, ya no es aceptable para un compilador C tradicional. El código debe ser compilado por un compilador que reconozca y comprenda estas declaraciones adicionales. El código del host es código ANSI C directo, que se compila adicionalmente con los compiladores C / C++ estándar del host y se ejecuta como un proceso de CPU tradicional. En situaciones donde no hay ningún dispositivo de hardware disponible o un kernel se puede ejecutar apropiadamente en una CPU, también se puede optar por ejecutar el kernel en una CPU usando herramientas como MCUDA [SSH 2008].
La ejecución comienza con el código del host (código de serie de la CPU). Cuando se llama o inicia una función del kernel (código de dispositivo paralelo), es ejecutada por una gran cantidad de subprocesos en un dispositivo. Todos los subprocesos generados por el lanzamiento de un kernel se denominan colectivamente grid. Estos subprocesos son el vehículo principal de ejecución paralela en una plataforma CUDA. Cuando todos los subprocesos de un kernel completan su ejecución, el grid correspondiente finaliza y la ejecución continúa en el host hasta que se inicia otro kernel.
El lanzamiento de un kernel normalmente genera una gran cantidad de subprocesos para explotar el paralelismo de datos. En el ejemplo de conversion de color a escala de grises, cada hilo podria usarse para calcular un pixel de la matriz de salida O. En este caso, el número de hilos que generará el núcleo es igual al número de píxeles de la imagen. En la práctica, cada hilo puede procesar múltiples pixeles para mayor eficiencia.

Threads.
Un hilo es una vista simplificada de cómo un procesador ejecuta un programa secuencial en las computadoras modernas. Un hilo consta del código del programa, el punto particular del código que se está ejecutando y los valores de sus variables y estructuras de datos. Se puede utilizar un depurador a nivel de fuente para monitorear el progreso de un hilo ejecutando una declaración a la vez, observando la declaración que se ejecutará a continuación y verificando los valores de las variables y estructuras de datos a medida que avanza la ejecución.
Los hilos se han utilizado en programación durante muchos años. Si un programador quiere iniciar la ejecución paralela en una aplicación, crea y administra múltiples subprocesos utilizando bibliotecas de subprocesos o lenguajes especiales.

A Vector Addition Kernel.
Ahora usamos la suma de vectores para ilustrar la estructura del programa CUDA C. La suma de vectores es posiblemente el cálculo paralelo de datos más simple posible, el equivalente paralelo de “Hola mundo” de la programación secuencial.
Suponga que los vectores que se van a agregar están almacenados en las matrices A y B que se asignan e inicializan en el programa principal. Por brevedad, no mostramos los detalles de cómo se asignan o inicializan A, B y C en la función principal. Los punteros a estas matrices se pasan a la función vecAdd, junto con la variable N que contiene la longitud de los vectores.
Una forma sencilla de ejecutar la suma de vector en paralelo es modificar la función vecAdd y mover sus cálculos a un dispositivo. Al principio del archivo, debemos agregar una directiva de preprocesador C para incluir el archivo de encabezado cuda.h. Este archivo define las funciones de la API CUDA y las variables integradas que presentaremos pronto. La parte 1 de la función asigna espacio en la memoria del dispositivo (GPU) para guardar copias de los vectores A, B y C y copia los vectores de la memoria del host a la memoria del dispositivo. La parte 2 inicia la ejecución paralela del núcleo de adición de vectores real en el dispositivo. La parte 3 copia el vector de suma C de la memoria del dispositivo a la memoria del host y libera los vectores en la memoria del dispositivo.
Tenga en cuenta que la función vecAdd revisada es esencialmente un agente de subcontratación que envía datos de entrada a un dispositivo, activa el cálculo en el dispositivo y recopila los resultados. El agente lo hace de tal manera que el programa principal ni siquiera necesita ser consciente de que la suma del vector ahora se realiza realmente en un dispositivo. En la práctica, este modelo de subcontratación “transparente” puede resultar muy ineficiente debido a toda la copia de datos de un lado a otro.

Device Global Memory And Data Transfer.
En los sistemas CUDA actuales, los dispositivos suelen ser tarjetas de hardware que vienen con su propia memoria dinámica de acceso aleatoria (DRAM). Por ejemplo, la NVIDIA GTX1080 viene con hasta 8 GB1 de DRAM, llamada memoria global. Para ejecutar un kernel en un dispositivo, el programador necesita asignar memoria global en el dispositivo y transferir datos pertinentes desde la memoria del host a la memoria asignada del dispositivo. De manera similar, después de la ejecución del dispositivo, el programador necesita transferir los datos de los resultados desde la memoria del dispositivo a la memoria del host y liberar la memoria del dispositivo que ya no es necesaria. El sistema de tiempo de ejecución CUDA proporciona funciones API para realizar estas actividades en nombre del programador.
El primer parámetro de la función cudaMalloc es la dirección de una variable de puntero que se configurara para que apunte al objeto asignado. La dirección de la variable del puntero se debe convertir a (void **) porque la función espera un puntero genérico; La función de asignación de memoria es una función genérica que no está restringida a ningún tipo particular de objetos. Este parámetro permite que la función cudaMalloc escriba la dirección de la memoria asignada en la variable de puntero.
Una vez que el código de host ha asignado memoria del dispositivo para los objetos de datos, puede solicitar que los datos se transfieran del host al dispositivo. Esto se logra llamando a una de las funciones de la API de CUDA. La función cudaMemcpy toma cuatro parámetros. El primer parámetro es un puntero a la ubicación de destino del objeto de datos que se va a copiar. El segundo parámetro apunta a la ubicación de origen. El tercer parámetro especifica el número de bytes que se copiaran. El cuarto parámetro indica los tipos de memoria involucrados en la copia: de memoria de host a memoria de host, de memoria de host a memoria de dispositivo, de memoria de dispositivo a memoria de host y de memoria de dispositivo a memoria de dispositivo.
La función vecAdd llama a la función cudaMemcpy para copiar los vectores h_A y h_B del host al dispositivo antes de agregarlos y para copiar al vector h_C del dispositivo al host una vez realizada la adición.

Built-In Variables.
Muchos lenguajes de programación tienen variables integradas. Estas variables tienen un significado y propósito especial. Los valores de estas variables a menudo son pre inicializados por el sistema de ejecución y normalmente son de solo lectura en el programa.

Kernel Functions And Threading.
En CUDA, una función del kernel especifica el código que ejecutan todos los subprocesos durante una fase paralela. Dado que todos estos subprocesos ejecutan el mismo código, la programación CUDA es un ejemplo del conocido estilo de programación paralela de programa único y datos múltiples (SPMD) [Ata 1998], un estilo de programación popular para sistemas informáticos masivamente paralelos.
Cuando el código host de un programa inicia un kernel, el sistema de ejecución de CUDA genera una cuadrícula de subprocesos que se organizan en una jerarquía de dos niveles. Cada cuadrícula está organizada como una serie de bloques de subprocesos, a los que nos referimos como bloques para abreviar. Todos los bloques de una cuadrícula son del mismo tamaño; cada bloque puede contener hasta 1024 subprocesos. El número total de subprocesos lo especifica el código del host cuando se inicia un kernel.
La variable blockDim es de tipo estructura con tres campos enteros sin signo: X, Y y Z, que ayudan al programador a organizar los subprocesos en una matriz de una, dos o tres dimensiones. Para una organización unidimensional, sólo se utilizará el campo X. Para una organización bidimensional, se utilizaran los campos X e Y. Para una estructura tridimensional, se utilizaran los tres campos. La elección de la dimensionalidad para organizar los hilos suele reflejar la dimensionalidad de los datos.
Los núcleos CUDA tienen acceso a dos variables integradas más (threadIdx, blockIdx) que permiten a los subprocesos distinguirse entre sí y determinar el área de datos en la que debe trabajar cada subproceso. La variable threadIdx le da a cada hilo una coordenada única dentro de un bloque.
En general, CUDA C extiende el lenguaje C con tres palabras clave calificadoras que se pueden usar en declaraciones de funciones. La palabra clave “_global_” indica que la función que se declara es una función del núcleo CUDA C. La palabra clave “_device_” indica que la función que se declara es una función de dispositivo CUDA.
La palabra clave “_host_” indica que la función que se declara es una función de host CUDA. Una función de host es simplemente una función de C tradicional que se ejecuta en el host y solo se puede llamar desde otra función de host. De forma predeterminada, todas las funciones de un programa CUDA son funciones de host si no tienen ninguna de las palabras clave CUDA en su declaración.
Tenga en cuenta que se pueden utilizar tanto “__host__” como “__device__” en una declaración de función. Esta combinación le dice al sistema de compilación que genere dos versiones de archivos objeto para la misma función. Uno se ejecuta en el host y solo se puede llamar desde una función del host.

Function Declarations.
CUDA C amplía la sintaxis de declaración de funciones de C para admitir computación paralela heterogénea. Usando uno de “__global__”, “__device__” o “__host__”, un programador de CUDA C puede indicarle al compilador que genere una función del núcleo, una función de dispositivo o una función de host. Todas las declaraciones de funciones sin ninguna de estas palabras clave utilizan de forma predeterminada funciones de host. Si se utilizan tanto “__host__” como “_device__” en una declaración de función, el compilador genera dos versiones de la función, una para el dispositivo y otra para el host.

Kernel Launch.
CUDA C extiende la sintaxis de llamada a funciones C con parámetros de configuración de ejecución del kernel rodeados por <<< y >>>. Estos parámetros de configuración de ejecución solo se utilizan durante una llamada a una función del kernel o durante el inicio del kernel.
